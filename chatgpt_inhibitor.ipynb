{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT Prefrontal Cortex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Copyright 2023, [Jozsef Szalma](https://www.linkedin.com/in/szalma/)<br>\n",
    "Creative Commons Attribution-NonCommercial 4.0 International Public License <br>\n",
    "https://creativecommons.org/licenses/by-nc/4.0/legalcode <br>\n",
    "If you want to use the below code commercially then how about you contact me first, thanks.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "\n",
    "#%pip install openai\n",
    "#%pip install ipywidgets\n",
    "#%pip install pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import pyperclip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api key\n",
    "openai.api_key = os.getenv(\"KEY\")\n",
    "\n",
    "#default personality / behavior for the chatbot (e.g. \"You are a helpful and knowledgable assistant\"); this is also where you might want to spell out the role of the inhibitor model, so the main model knows what to do with it\n",
    "default_system = os.getenv(\"PROMPT_SYSTEM\")\n",
    "\n",
    "#default instructions to the inhibitor model (i.e. what to look for, formatting the feedback in json etc)\n",
    "#the below code is expecting the responses from the inhibitor in the following format: \n",
    "# {\"decision\": \"pass\", \"explanation\": \"\"}\n",
    "# {\"decision\": \"inhibit\", \"explanation\": \"\"}\n",
    "#so you need to engineer your prompt accordingly\n",
    "default_inhibitor = os.getenv(\"PROMPT_INHIBITOR\")\n",
    "\n",
    "#chat model parameters (ChatGPT)\n",
    "chat_model = \"gpt-3.5-turbo\"\n",
    "chat_temp = 0.7\n",
    "chat_convo_limit = 3000\n",
    "chat_token_limit = 1000\n",
    "\n",
    "#inhibitor model parameters (GPT3)\n",
    "inhibitor_model = \"text-davinci-003\"\n",
    "inhibitor_temp = 0.0\n",
    "inhibitor_token_limit = 1000\n",
    "\n",
    "#TODO refactor to get rid of global variables\n",
    "\n",
    "if (default_system is not None):\n",
    "    #initializing chat history with the system prompt\n",
    "    conversation = [{\"role\": \"system\", \"content\": default_system}]\n",
    "else:\n",
    "    conversation = []\n",
    "    \n",
    "#the maximum context window (short term memory) is 4096 tokens, so we need to keep track of token usage to limit the risk running into an API error; the web UI is using a sliding window to manage around this limit\n",
    "token_count = 0\n",
    "\n",
    "\n",
    "#API calls to OpenAI\n",
    "def get_gpt (prompt):\n",
    "    global token_count\n",
    "    global conversation\n",
    "\n",
    "    #if our current token_count is above limit, then we remove the oldest message from the conversation, if there is a personality set up in default_system then the 2nd oldest\n",
    "    if token_count > chat_convo_limit:\n",
    "   \n",
    "        prompt_length = len(prompt)\n",
    "\n",
    "        while prompt_length > 0 :\n",
    "   \n",
    "            if (default_system is not None):\n",
    "                item_length = len(conversation[1]['content'])\n",
    "                conversation.pop(1)\n",
    "            else:\n",
    "                item_length = len(conversation[0]['content'])\n",
    "                conversation.pop(0)\n",
    "\n",
    "            prompt_length = prompt_length - item_length\n",
    "\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    #sending user prompt to primary model\n",
    "    response = openai.ChatCompletion.create(\n",
    "            model=chat_model, \n",
    "            messages=conversation,\n",
    "            temperature = chat_temp,\n",
    "            max_tokens = chat_token_limit,\n",
    "            n = 1\n",
    "\n",
    "        )\n",
    "    message = response.choices[0].message.content    \n",
    "    conversation.append({\"role\": \"assistant\", \"content\": message}) \n",
    "    \n",
    "    #sending instructions to inhibitor model + primary model's last response. \n",
    "    #it is a design decision if the entire chat history should be sent to the inhibitor (more context) or only the last message (less bias?)\n",
    "\n",
    "    inhibitor_response = openai.Completion.create(\n",
    "            model = inhibitor_model,\n",
    "            prompt = default_inhibitor + \" \" + message,\n",
    "            temperature = chat_temp,\n",
    "            max_tokens = inhibitor_token_limit,\n",
    "            n = 1\n",
    "        )\n",
    "    \n",
    "    \n",
    "    inhibitor_message = inhibitor_response.choices[0].text \n",
    "    #the inhibitor's responses are added to the conversation for transparency\n",
    "    conversation.append({\"role\": \"system\", \"content\": inhibitor_message})   \n",
    "    try:\n",
    "        inhibitor_decision = json.loads(inhibitor_message)['decision']\n",
    "\n",
    "        #if the inhibitor has an issue the primary model gets another opportunity to rephrase its response.\n",
    "    \n",
    "        if (inhibitor_decision.lower() == 'inhibit'):\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=chat_model, \n",
    "                messages=conversation,\n",
    "                temperature = chat_temp,\n",
    "                max_tokens = chat_token_limit,\n",
    "                n = 1\n",
    "\n",
    "            )\n",
    "            message = response.choices[0].message.content    \n",
    "        \n",
    "            conversation.append({\"role\": \"assistant\", \"content\": message}) \n",
    "    except:\n",
    "        print('inhibitor returned malformed response: ', inhibitor_message)    \n",
    "    token_count = response.usage.total_tokens\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#HTML formatting for the output window\n",
    "html_template = \"\"\"\n",
    "<style>\n",
    "div {{white-space: pre-wrap;font-size: 18px;}}\n",
    "\n",
    "</style>\n",
    "<div>\n",
    "<div>\n",
    "{}\n",
    "</div>\n",
    "<div>\n",
    "{}\n",
    "</div>\n",
    "<div>\n",
    "{}\n",
    "</div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "#displaying messages\n",
    "def render_message(message):\n",
    "    if message[\"role\"] == \"assistant\":\n",
    "        return (f\"<i>ChatGPT</i>: {message['content']}\")\n",
    "    elif message[\"role\"] == \"system\":\n",
    "        return (f\"<i>System</i>: {message['content']}\")\n",
    "    elif message[\"role\"] == \"user\":\n",
    "        return (f\"<b>You</b>: {message['content']}\")\n",
    "    \n",
    "# handler for copy button's on_click event\n",
    "def handle_copy(button):\n",
    "    global conversation\n",
    "    \n",
    "    if (default_system is not None):\n",
    "        conversation_str = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in conversation[1:]])\n",
    "    else:\n",
    "        conversation_str = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in conversation])\n",
    "\n",
    "    # Add conversation string to clipboard\n",
    "    pyperclip.copy(conversation_str)\n",
    "\n",
    "# handler for send button's on_click event\n",
    "def handle_input(button):\n",
    "    user_input = input_widget.value\n",
    "    output.append_display_data(\"--------------message sent--------------\")\n",
    "    input_widget.value = ''\n",
    "    \n",
    "    response = get_gpt(user_input)\n",
    "    \n",
    "    latest_messages = html_template.format(render_message(conversation[-3]),render_message(conversation[-2]),render_message(conversation[-1]))\n",
    "    output.append_display_data(HTML(latest_messages))\n",
    "\n",
    "    print('current token count: ', token_count)\n",
    "\n",
    "\n",
    "output = widgets.Output(layout={'border': '1px solid black','width': '100%'})\n",
    "\n",
    "#input window\n",
    "input_widget = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type your message here...',\n",
    "    description='Input:',\n",
    "    disabled=False,\n",
    "    layout={'width': '100%', 'height': 'auto','font-size':'20px'},\n",
    "    rows = 1\n",
    ")\n",
    "\n",
    "#resizing input window as the input gets longer    \n",
    "def get_bigger(args):        \n",
    "    input_widget.rows = input_widget.value.count('\\n') + 1\n",
    "\n",
    "input_widget.observe(get_bigger,'value')\n",
    "\n",
    "#the send button\n",
    "send_button = widgets.Button(description='Send', layout={'width': '100%'})\n",
    "send_button.on_click(handle_input)\n",
    "\n",
    "#the copy to clipboard button\n",
    "copy_button = widgets.Button(description='Copy Chat to Clipboard', layout={'width': '100%'})\n",
    "copy_button.on_click(handle_copy)\n",
    "\n",
    "\n",
    "# Display the UI\n",
    "display(widgets.VBox([copy_button,output,input_widget,send_button]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
